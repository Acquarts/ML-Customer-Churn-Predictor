# =============================================================================
# CHURN PREDICTION PROJECT CONFIGURATION
# =============================================================================
# This file contains all configurable parameters for the ML pipeline.
# Modify values here instead of hardcoding in source files.

project:
  name: "Customer Churn Prediction"
  version: "1.0.0"
  author: "Your Name"
  description: "End-to-end ML system for predicting customer churn"

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
data:
  # Data source (kaggle dataset ID or local path)
  source: "blastchar/telco-customer-churn"
  
  # Paths (relative to project root)
  raw_path: "data/raw/telco_churn.csv"
  processed_path: "data/processed/telco_churn_processed.csv"
  
  # Target variable
  target_column: "Churn"
  
  # Identifier column
  id_column: "customerID"
  
  # Train/Test split
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  stratify: true

# =============================================================================
# FEATURE ENGINEERING
# =============================================================================
features:
  # Numerical features
  numerical:
    - "tenure"
    - "MonthlyCharges"
    - "TotalCharges"
  
  # Categorical features
  categorical:
    - "gender"
    - "SeniorCitizen"
    - "Partner"
    - "Dependents"
    - "PhoneService"
    - "MultipleLines"
    - "InternetService"
    - "OnlineSecurity"
    - "OnlineBackup"
    - "DeviceProtection"
    - "TechSupport"
    - "StreamingTV"
    - "StreamingMovies"
    - "Contract"
    - "PaperlessBilling"
    - "PaymentMethod"
  
  # Encoding strategy
  encoding:
    method: "target"  # Options: onehot, target, label
    handle_unknown: "value"
  
  # Scaling strategy
  scaling:
    method: "standard"  # Options: standard, minmax, robust
  
  # Feature engineering flags
  create_interactions: true
  create_polynomial: false
  polynomial_degree: 2

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
models:
  # Models to train
  train_models:
    - "logistic_regression"
    - "random_forest"
    - "xgboost"
    - "lightgbm"
  
  # Primary metric for model selection
  primary_metric: "roc_auc"  # Options: accuracy, precision, recall, f1, roc_auc
  
  # Class imbalance handling
  handle_imbalance: true
  imbalance_method: "smote"  # Options: smote, adasyn, class_weight
  
  # Hyperparameter configurations
  hyperparameters:
    logistic_regression:
      C: [0.01, 0.1, 1, 10]
      penalty: ["l1", "l2"]
      solver: ["saga"]
      max_iter: 1000
    
    random_forest:
      n_estimators: [100, 200, 300]
      max_depth: [5, 10, 15, null]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
      class_weight: ["balanced", null]
    
    xgboost:
      n_estimators: [100, 200, 300]
      max_depth: [3, 5, 7]
      learning_rate: [0.01, 0.05, 0.1]
      subsample: [0.8, 0.9, 1.0]
      colsample_bytree: [0.8, 0.9, 1.0]
      scale_pos_weight: [1, 3, 5]
    
    lightgbm:
      n_estimators: [100, 200, 300]
      max_depth: [3, 5, 7]
      learning_rate: [0.01, 0.05, 0.1]
      num_leaves: [31, 50, 70]
      class_weight: ["balanced", null]

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  # Cross-validation
  cv_folds: 5
  cv_strategy: "stratified"  # Options: stratified, kfold, timeseries
  
  # Hyperparameter optimization
  hp_optimization:
    method: "optuna"  # Options: grid, random, optuna
    n_trials: 50
    timeout: 3600  # seconds
  
  # Early stopping (for gradient boosting)
  early_stopping:
    enabled: true
    rounds: 50
    
  # Model artifacts
  save_path: "models/"
  model_name: "best_model.pkl"
  
  # MLflow tracking
  mlflow:
    enabled: true
    experiment_name: "churn_prediction"
    tracking_uri: "mlruns/"

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================
evaluation:
  # Metrics to compute
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
    - "average_precision"
    - "log_loss"
  
  # Classification threshold
  threshold: 0.5
  optimize_threshold: true
  
  # Reports
  generate_reports: true
  reports_path: "reports/"
  
  # Plots to generate
  plots:
    - "confusion_matrix"
    - "roc_curve"
    - "precision_recall_curve"
    - "feature_importance"
    - "shap_summary"
    - "calibration_curve"

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================
deployment:
  # Streamlit app
  streamlit:
    title: "Customer Churn Predictor"
    page_icon: "ðŸ”®"
    layout: "wide"
    
  # API (if using FastAPI)
  api:
    host: "0.0.0.0"
    port: 8000
    
  # Docker
  docker:
    image_name: "churn-predictor"
    tag: "latest"

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/pipeline.log"
