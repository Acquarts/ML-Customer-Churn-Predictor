{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Model Training & Evaluation\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Date:** 2024  \n",
    "**Objective:** Train, evaluate, and select the best model for churn prediction\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#1.-Setup)\n",
    "2. [Data Preparation](#2.-Data-Preparation)\n",
    "3. [Baseline Models](#3.-Baseline-Models)\n",
    "4. [Advanced Models](#4.-Advanced-Models)\n",
    "5. [Model Comparison](#5.-Model-Comparison)\n",
    "6. [Hyperparameter Tuning](#6.-Hyperparameter-Tuning)\n",
    "7. [Final Evaluation](#7.-Final-Evaluation)\n",
    "8. [Model Interpretability](#8.-Model-Interpretability)\n",
    "9. [Save Best Model](#9.-Save-Best-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report, average_precision_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Advanced models\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "# Project modules\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.preprocessing import DataPreprocessor\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(f\"   XGBoost available: {XGBOOST_AVAILABLE}\")\n",
    "print(f\"   LightGBM available: {LIGHTGBM_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = DataLoader(config_path=str(project_root / 'config' / 'config.yaml'))\n",
    "df = loader.load_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor(config_path=str(project_root / 'config' / 'config.yaml'))\n",
    "X_train, X_test, y_train, y_test = preprocessor.fit_transform(df)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTarget distribution (train):\")\n",
    "print(f\"  Class 0 (No Churn): {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Class 1 (Churn): {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names()\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"\\nFirst 10 features: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, returning metrics.\"\"\"\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    return model, metrics, y_pred, y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, random_state=RANDOM_STATE, class_weight='balanced'\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        max_depth=5, random_state=RANDOM_STATE, class_weight='balanced'\n",
    "    ),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = []\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    trained_model, metrics, y_pred, y_proba = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test, name\n",
    "    )\n",
    "    results.append(metrics)\n",
    "    trained_models[name] = trained_model\n",
    "    predictions[name] = {'y_pred': y_pred, 'y_proba': y_proba}\n",
    "    \n",
    "    print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  CV Score: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced models\n",
    "advanced_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, random_state=RANDOM_STATE,\n",
    "        class_weight='balanced', n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    advanced_models['XGBoost'] = XGBClassifier(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        scale_pos_weight=3, random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    advanced_models['LightGBM'] = LGBMClassifier(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        class_weight='balanced', random_state=RANDOM_STATE, verbose=-1\n",
    "    )\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    trained_model, metrics, y_pred, y_proba = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test, name\n",
    "    )\n",
    "    results.append(metrics)\n",
    "    trained_models[name] = trained_model\n",
    "    predictions[name] = {'y_pred': y_pred, 'y_proba': y_proba}\n",
    "    \n",
    "    print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  CV Score: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results DataFrame\n",
    "results_df = pd.DataFrame(results).set_index('model')\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('roc_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model comparison\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric.upper(),\n",
    "        x=results_df.index,\n",
    "        y=results_df[metric],\n",
    "        text=[f'{v:.3f}' for v in results_df[metric]],\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison',\n",
    "    barmode='group',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', y=1.15)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "for idx, (name, preds) in enumerate(predictions.items()):\n",
    "    if preds['y_proba'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, preds['y_proba'])\n",
    "        auc = roc_auc_score(y_test, preds['y_proba'])\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=fpr, y=tpr,\n",
    "            name=f'{name} (AUC={auc:.3f})',\n",
    "            mode='lines',\n",
    "            line=dict(color=colors[idx % len(colors)], width=2)\n",
    "        ))\n",
    "\n",
    "# Diagonal line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    line=dict(color='gray', dash='dash'),\n",
    "    name='Random Classifier'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curves Comparison',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=500,\n",
    "    legend=dict(x=0.6, y=0.1)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, (name, preds) in enumerate(predictions.items()):\n",
    "    if preds['y_proba'] is not None:\n",
    "        precision, recall, _ = precision_recall_curve(y_test, preds['y_proba'])\n",
    "        ap = average_precision_score(y_test, preds['y_proba'])\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=recall, y=precision,\n",
    "            name=f'{name} (AP={ap:.3f})',\n",
    "            mode='lines',\n",
    "            line=dict(color=colors[idx % len(colors)], width=2)\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Precision-Recall Curves Comparison',\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for tuning\n",
    "best_model_name = results_df['roc_auc'].idxmax()\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"ROC-AUC: {results_df.loc[best_model_name, 'roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter tuning with Optuna\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Optuna objective function for XGBoost.\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        return scores.mean()\n",
    "    \n",
    "    # Run optimization (limited trials for demo)\n",
    "    print(\"Running hyperparameter optimization...\")\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nBest ROC-AUC: {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    # Train with best params\n",
    "    best_params = study.best_params\n",
    "    best_params['random_state'] = RANDOM_STATE\n",
    "    best_params['use_label_encoder'] = False\n",
    "    best_params['eval_metric'] = 'logloss'\n",
    "    \n",
    "    tuned_model = XGBClassifier(**best_params)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Optuna not available. Using default hyperparameters.\")\n",
    "    tuned_model = trained_models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Final predictions\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "y_proba_final = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"=\"*60)\n",
    "print(f\"FINAL MODEL EVALUATION: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted: No', 'Predicted: Yes'],\n",
    "    y=['Actual: No', 'Actual: Yes'],\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={'size': 20},\n",
    "    colorscale='Blues',\n",
    "    showscale=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Confusion Matrix - {best_model_name}',\n",
    "    height=400,\n",
    "    width=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives:  {tn} (correctly identified non-churners)\")\n",
    "print(f\"  False Positives: {fp} (incorrectly flagged as churners)\")\n",
    "print(f\"  False Negatives: {fn} (missed churners - COSTLY!)\")\n",
    "print(f\"  True Positives:  {tp} (correctly identified churners)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = best_model.feature_importances_\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    importance = np.abs(best_model.coef_[0])\n",
    "else:\n",
    "    importance = None\n",
    "\n",
    "if importance is not None:\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    # Plot\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=importance_df['importance'],\n",
    "        y=importance_df['feature'],\n",
    "        orientation='h',\n",
    "        marker_color='steelblue'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Top 15 Feature Importance - {best_model_name}',\n",
    "        xaxis_title='Importance',\n",
    "        yaxis_title='Feature',\n",
    "        height=500,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis (if available)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    print(\"Computing SHAP values...\")\n",
    "    \n",
    "    # Sample for efficiency\n",
    "    sample_idx = np.random.choice(len(X_test), size=min(100, len(X_test)), replace=False)\n",
    "    X_sample = X_test[sample_idx]\n",
    "    \n",
    "    # Create explainer\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "    else:\n",
    "        explainer = shap.LinearExplainer(best_model, X_train)\n",
    "    \n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Handle different shap_values formats\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # For class 1 (Churn)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Feature Importance - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "# Create models directory\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = models_dir / 'best_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor.save(str(models_dir / 'preprocessor.pkl'))\n",
    "print(f\"âœ… Preprocessor saved\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'metrics': {\n",
    "        'accuracy': float(results_df.loc[best_model_name, 'accuracy']),\n",
    "        'precision': float(results_df.loc[best_model_name, 'precision']),\n",
    "        'recall': float(results_df.loc[best_model_name, 'recall']),\n",
    "        'f1': float(results_df.loc[best_model_name, 'f1']),\n",
    "        'roc_auc': float(results_df.loc[best_model_name, 'roc_auc']),\n",
    "        'cv_mean': float(results_df.loc[best_model_name, 'cv_mean']),\n",
    "        'cv_std': float(results_df.loc[best_model_name, 'cv_std'])\n",
    "    },\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'n_train_samples': len(X_train),\n",
    "    'n_test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "metadata_path = models_dir / 'best_model_metadata.yaml'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    yaml.dump(metadata, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ… Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  â€¢ Accuracy:  {metadata['metrics']['accuracy']:.4f}\")\n",
    "print(f\"  â€¢ Precision: {metadata['metrics']['precision']:.4f}\")\n",
    "print(f\"  â€¢ Recall:    {metadata['metrics']['recall']:.4f}\")\n",
    "print(f\"  â€¢ F1 Score:  {metadata['metrics']['f1']:.4f}\")\n",
    "print(f\"  â€¢ ROC-AUC:   {metadata['metrics']['roc_auc']:.4f}\")\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"  1. Run the Streamlit app: streamlit run app/streamlit_app.py\")\n",
    "print(\"  2. Test predictions with new data\")\n",
    "print(\"  3. Monitor model performance over time\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
